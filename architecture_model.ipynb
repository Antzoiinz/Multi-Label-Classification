{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Liste complète des labels possibles\u001b[39;00m\n\u001b[0;32m     20\u001b[0m all_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m labels \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf_train\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverified_uat_labels\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(labels, (\u001b[38;5;28mlist\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray)):  \u001b[38;5;66;03m# Vérifier si les labels sont sous forme de liste ou tableau numpy\u001b[39;00m\n\u001b[0;32m     24\u001b[0m         all_labels\u001b[38;5;241m.\u001b[39mupdate(labels\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(labels, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m labels)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_train' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Charger les données\n",
    "data = pd.read_parquet(r'C:\\Users\\antoi\\OneDrive\\Documents\\IPSA\\Aero_4\\Ma412\\project\\train-00000-of-00001-b21313e511aa601a.parquet')\n",
    "\n",
    "# Prétraitement des abstracts et labels\n",
    "data['abstract'] = data['abstract'].fillna('')  # Remplacer les valeurs manquantes par des chaînes vides\n",
    "data['verified_uat_labels'] = data['verified_uat_labels'].apply(lambda x: x.tolist() if isinstance(x, (list, np.ndarray)) else [])\n",
    "\n",
    "# Liste complète des labels possibles\n",
    "all_labels = set()\n",
    "\n",
    "for labels in data['verified_uat_labels']:\n",
    "    if isinstance(labels, (list, np.ndarray)):  # Vérifier si les labels sont sous forme de liste ou tableau numpy\n",
    "        all_labels.update(labels.tolist() if isinstance(labels, np.ndarray) else labels)\n",
    "    else:\n",
    "        print(f\"Format inattendu pour les labels : {labels}\")\n",
    "\n",
    "all_possible_labels = sorted(all_labels)\n",
    "\n",
    "# MultiLabelBinarizer avec la liste complète des labels possibles\n",
    "mlb = MultiLabelBinarizer(classes=all_possible_labels)\n",
    "mlb.fit(all_possible_labels)\n",
    "\n",
    "# Transformation des labels dans les données\n",
    "Y = mlb.transform(data['verified_uat_labels'])\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train_raw, X_test_raw, Y_train, Y_test = train_test_split(data['abstract'], Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorisation avec TfidfVectorizer (sublinear TF)\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english', sublinear_tf=True)\n",
    "X_train = vectorizer.fit_transform(X_train_raw)\n",
    "X_test = vectorizer.transform(X_test_raw)\n",
    "\n",
    "# Fonction pour tester différentes architectures de modèles\n",
    "def test_model_architectures():\n",
    "    models = {\n",
    "        \"Logistic Regression (default)\": LogisticRegression(max_iter=2000, class_weight='balanced'),\n",
    "        \"Logistic Regression (C=0.5)\": LogisticRegression(max_iter=2000, class_weight='balanced', C=0.5),\n",
    "        \"Logistic Regression (C=2.0)\": LogisticRegression(max_iter=2000, class_weight='balanced', C=2.0),\n",
    "        \"Support Vector Machine (default)\": LinearSVC(class_weight='balanced', max_iter=2000),\n",
    "        \"Support Vector Machine (C=0.5)\": LinearSVC(class_weight='balanced', max_iter=2000, C=0.5),\n",
    "        \"Support Vector Machine (C=2.0)\": LinearSVC(class_weight='balanced', max_iter=2000, C=2.0),\n",
    "        \"Naive Bayes (default)\": MultinomialNB(),\n",
    "        \"Naive Bayes (alpha=0.5)\": MultinomialNB(alpha=0.5),\n",
    "        \"Naive Bayes (alpha=1.5)\": MultinomialNB(alpha=1.5)\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n=== Testing {name} ===\")\n",
    "        # Entraîner le modèle\n",
    "        classifier = OneVsRestClassifier(model)\n",
    "        classifier.fit(X_train, Y_train)\n",
    "\n",
    "        # Évaluer le modèle\n",
    "        Y_pred = classifier.predict(X_test)\n",
    "        accuracy = accuracy_score(Y_test, Y_pred)\n",
    "        f1_micro = f1_score(Y_test, Y_pred, average='micro')\n",
    "        f1_macro = f1_score(Y_test, Y_pred, average='macro')\n",
    "\n",
    "        print(\"Rapport de classification :\")\n",
    "        print(classification_report(Y_test, Y_pred, target_names=mlb.classes_))\n",
    "\n",
    "        results[name] = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"f1_micro\": f1_micro,\n",
    "            \"f1_macro\": f1_macro,\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "# Tester les architectures de modèles\n",
    "results = test_model_architectures()\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"\\n=== Résultats des architectures de modèles ===\")\n",
    "for method, metrics in results.items():\n",
    "    print(f\"\\nModèle : {method}\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric.capitalize()} : {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
