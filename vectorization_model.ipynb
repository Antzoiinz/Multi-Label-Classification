{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pca_implementation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulticlass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OneVsRestClassifier\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, accuracy_score, f1_score\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpca_implementation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[0;32m     14\u001b[0m df_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mantoi\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mIPSA\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAero_4\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMa412\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mproject\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtrain-00000-of-00001-b21313e511aa601a.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mantoi\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mIPSA\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAero_4\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mMa412\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mproject\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mval-00000-of-00001-66ce8665444026dc.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pca_implementation'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from pca_implementation import PCA\n",
    "\n",
    "\n",
    "df_train = pd.read_parquet(r'C:\\Users\\antoi\\OneDrive\\Documents\\IPSA\\Aero_4\\Ma412\\project\\train-00000-of-00001-b21313e511aa601a.parquet')\n",
    "df = pd.read_parquet(r'C:\\Users\\antoi\\OneDrive\\Documents\\IPSA\\Aero_4\\Ma412\\project\\val-00000-of-00001-66ce8665444026dc.parquet')\n",
    "print(np.shape(df_train))\n",
    "# Extraire les labels uniques\n",
    "all_labels = set()\n",
    "\n",
    "for labels in df_train['verified_uat_labels']:\n",
    "    if isinstance(labels, (list, np.ndarray)):  # Vérifier si les labels sont sous forme de liste ou tableau numpy\n",
    "        all_labels.update(labels.tolist() if isinstance(labels, np.ndarray) else labels)\n",
    "    else:\n",
    "        print(f\"Format inattendu pour les labels : {labels}\")\n",
    "\n",
    "# Afficher tous les labels uniques\n",
    "unique_labels = sorted(all_labels)\n",
    "print(f\"Nombre de labels uniques : {len(unique_labels)}\")\n",
    "print(unique_labels)\n",
    "\n",
    "# Prétraitement des abstracts et labels\n",
    "df_train['abstract'] = df_train['abstract'].fillna('')  # Remplacer les valeurs manquantes par des chaînes vides\n",
    "df_train['verified_uat_labels'] = df_train['verified_uat_labels'].apply(lambda x: x.tolist() if isinstance(x, (list, np.ndarray)) else [])\n",
    "\n",
    "\n",
    "\n",
    "# MultiLabelBinarizer avec la liste complète des labels possibles\n",
    "mlb = MultiLabelBinarizer(classes=unique_labels)\n",
    "mlb.fit(unique_labels)\n",
    "\n",
    "# Transformation des labels dans les données\n",
    "Y = mlb.transform(df_train['verified_uat_labels'])\n",
    "\n",
    "# Vectorisation des abstracts (CountVectorizer)\n",
    "# Case 1\n",
    "vectorizer1 = CountVectorizer(max_features=5000, stop_words='english')  # Limiter à 5000 caractéristiques\n",
    "X1 = vectorizer1.fit_transform(df_train['abstract'])\n",
    "\n",
    "# Case 2\n",
    "vectorizer2 = TfidfVectorizer(max_features=5000)\n",
    "X2 = vectorizer2.fit_transform(df_train['abstract'])\n",
    "\n",
    "# Case 3\n",
    "vectorizer3 = TfidfVectorizer(max_features=5000,max_df = 0.8, min_df = 0.1)\n",
    "X3 = vectorizer3.fit_transform(df_train['abstract'])\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "# Case 1\n",
    "X_train1, X_test1, Y_train, Y_test = train_test_split(X1, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Case 2\n",
    "X_train2, X_test2, Y_train, Y_test = train_test_split(X2, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Case 3\n",
    "X_train3, X_test3, Y_train, Y_test = train_test_split(X3, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Filtrer les colonnes correspondant aux labels toujours présents\n",
    "indices_to_keep = [i for i, label in enumerate(mlb.classes_) if label not in always_present_labels]\n",
    "Y_train = Y_train[:, indices_to_keep]\n",
    "Y_test = Y_test[:, indices_to_keep]\n",
    "mlb.classes_ = mlb.classes_[indices_to_keep]\n",
    "\n",
    "# Entraîner le modèle de régression logistique et evaluer le modèle\n",
    "\n",
    "# Case 1\n",
    "logreg = LogisticRegression(max_iter=2000, class_weight='balanced')\n",
    "classifier1 = OneVsRestClassifier(logreg)\n",
    "classifier1.fit(X_train1, Y_train)\n",
    "Y_pred1 = (classifier1.predict_proba(X_test1) > 0.3).astype(int)  # Seuil de 30% pour la prédiction\n",
    "print(\"Rapport de classification :\")\n",
    "print(classification_report(Y_test, Y_pred1, target_names=mlb.classes_))\n",
    "\n",
    "# Calcul de la précision moyenne sur tous les labels\n",
    "accuracy1 = accuracy_score(Y_test, Y_pred1)\n",
    "f1_micro1 = f1_score(Y_test, Y_pred1, average='micro')\n",
    "f1_macro1 = f1_score(Y_test, Y_pred1, average='macro')\n",
    "\n",
    "print(f\"Précision globale (Accuracy) : {accuracy1:.4f}\")\n",
    "print(f\"F1-score (micro) : {f1_micro1:.4f}\")\n",
    "print(f\"F1-score (macro) : {f1_macro1:.4f}\")\n",
    "\n",
    "# Case 2\n",
    "logreg = LogisticRegression(max_iter=2000, class_weight='balanced')\n",
    "classifier2 = OneVsRestClassifier(logreg)\n",
    "classifier2.fit(X_train2, Y_train)\n",
    "Y_pred2 = (classifier2.predict_proba(X_test2) > 0.3).astype(int)  # Seuil de 30% pour la prédiction\n",
    "print(\"Rapport de classification :\")\n",
    "print(classification_report(Y_test, Y_pred1, target_names=mlb.classes_))\n",
    "\n",
    "# Calcul de la précision moyenne sur tous les labels\n",
    "accuracy2 = accuracy_score(Y_test, Y_pred2)\n",
    "f1_micro2 = f1_score(Y_test, Y_pred2, average='micro')\n",
    "f1_macro2 = f1_score(Y_test, Y_pred2, average='macro')\n",
    "\n",
    "print(f\"Précision globale (Accuracy) : {accuracy2:.4f}\")\n",
    "print(f\"F1-score (micro) : {f1_micro2:.4f}\")\n",
    "print(f\"F1-score (macro) : {f1_macro2:.4f}\")\n",
    "\n",
    "# Case 3\n",
    "logreg = LogisticRegression(max_iter=2000, class_weight='balanced')\n",
    "classifier3 = OneVsRestClassifier(logreg)\n",
    "classifier3.fit(X_train3, Y_train)\n",
    "Y_pred3 = (classifier3.predict_proba(X_test3) > 0.3).astype(int)  # Seuil de 30% pour la prédiction\n",
    "print(\"Rapport de classification :\")\n",
    "print(classification_report(Y_test, Y_pred3, target_names=mlb.classes_))\n",
    "\n",
    "# Calcul de la précision moyenne sur tous les labels\n",
    "accuracy3 = accuracy_score(Y_test, Y_pred3)\n",
    "f1_micro3 = f1_score(Y_test, Y_pred3, average='micro')\n",
    "f1_macro3 = f1_score(Y_test, Y_pred3, average='macro')\n",
    "\n",
    "print(f\"Précision globale (Accuracy) : {accuracy3:.4f}\")\n",
    "print(f\"F1-score (micro) : {f1_micro3:.4f}\")\n",
    "print(f\"F1-score (macro) : {f1_macro3:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
